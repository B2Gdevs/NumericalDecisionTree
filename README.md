This is a decision tree that takes numerical data except for the last column of data which should be nominal.  This decision tree is for classification only.  Surprisingly it has good accuracy on the iris dataset roughly 80%-91%.  The accuracy is calculated by using 10-fold cross-validation.  The accuracy will be shown when the program is run.

This decision tree uses entropy-based binning to decided the best threshold for each split.  The best split is determined by the greatest information gained from the threshold.  As of right now only 10 thresholds are generated by taking the sum of each feature and dviding them by 10.  

The decision tree is formed as a binary tree where the left nodes are always set as less than or equal to the threshold specified for the feature being tested.  All leaf nodes contain a classification and a threshold of 0.  

The tree is generated recursively and it is printed recursively.  The program will automatically save the output of the initial tree (not the ones generated by the k-fold cross validation) and save it to a text file called decisionTree.txt.  It will save in the same location as the python file.

How to use:
 
In the commandline go to the location where the python file exists and then type....

python decisionTree.py "yourCSVFileHere.csv" 1 2 3

1 2 3 being the first, second, and third feature columns you want.  If you specify 0 for the first column it will not work.  This was intially made.

Difficulties:

An insane amount of difficulties. 
1. I didn't know Pandas very well and it was taxing to learn, but worth it as it will speed up processing in the future.  The dataframe object is made of series objects and little did I know that for a while.  Also I didn't know that pandas had helpful functions to help me get the true count of thresholds far eaiser than the way I implemented which was with a for loop.  I could have just made a boolean lambda function on a feature column and put it in a new column and then called the values_count() method for that column.
2. I was getting a lot of None types when building the tree recursively.  I just forgot to rename variables.
3. Keeping track of all the entropiesa generated from the entropy-based binning was very difficult, especially in a recursive tree.
4. Visualizing the tree.  However, it wasn't difficult once I had more guidance on the level of detail in needed to be.  I at first thought it had to rival the likes of ete's phylogenetic trees.  
5. Recursive prediction requires all methods to called to return something, not just the last caller.


All in all, it was a good experience even though it took a long time and got me behind on a lot of other work.
